{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to TensorFlow\n",
    "\n",
    "> Fully Connected Deep Networks with TensorFlow\n",
    "\n",
    "郭耀仁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 大綱\n",
    "\n",
    "- 關於 Fully Connected Deep Networks\n",
    "- 取得資料\n",
    "- Benchmark\n",
    "- 建構 TensorFlow 計算圖形\n",
    "- 訓練\n",
    "- 隨堂練習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 關於 Fully Connected Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fully Connected Deep Networks 多層感知器\n",
    "\n",
    "單一個感知器（Perceptron）的公式：\n",
    "\n",
    "$$y = \\sigma(WX + b)$$\n",
    "\n",
    "$\\sigma$ 即所謂的激活函數（activation function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 激活函數用來將線性關係映射至非線性關係\n",
    "\n",
    "常用的激活函數：\n",
    "- Sigmoid\n",
    "- TanH\n",
    "- ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 連結多個感知器建構出多層感知器，透過 propagations 求出係數\n",
    "\n",
    "- 輸入層\n",
    "- 隱藏層\n",
    "- 輸出層"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 什麼是 propagations（傳播）？\n",
    "\n",
    "- Forward Propagation: 第一個隨機猜測\n",
    "- Backward Propagation：訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/0501.png)\n",
    "\n",
    "Source: <https://math.stackexchange.com/questions/2048722/a-name-for-layered-directed-graph-as-in-a-fully-connected-neural-network>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 動手做一個 Fully Connected Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 先做 Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neural_Network:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "        self.W0 = np.random.randn(self._input_size, self._hidden_size)\n",
    "        self.W1 = np.random.randn(self._hidden_size, self._output_size)\n",
    "    def forward(self, X):\n",
    "        self.z0 = np.dot(X, self.W0)\n",
    "        self.z1 = self.sigmoid(self.z0)\n",
    "        self.z2 = np.dot(self.z1, self.W1)\n",
    "        output = self.sigmoid(self.z2)\n",
    "        return output\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 給定的 $X$ 與 $y$\n",
    "\n",
    "- 神經網路重視資料的標準化（Standard Scaling）\n",
    "    - 使用了 Activation Function 的緣故\n",
    "    - Standard Scaler\n",
    "    - MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def min_max_scaler(x):\n",
    "    x_min = x.min(axis=0)\n",
    "    x_max = x.max(axis=0)\n",
    "    return (x - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1)\n",
      "(30, 1)\n",
      "======\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]]\n",
      "======\n",
      "[[ 5.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [13.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.linspace(1, 30, 30).reshape(-1, 1)\n",
    "y = 2*X + 3\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(\"======\")\n",
    "print(X[:5])\n",
    "print(\"======\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.03448276]\n",
      " [0.06896552]\n",
      " [0.10344828]\n",
      " [0.13793103]]\n",
      "======\n",
      "[[0.        ]\n",
      " [0.03448276]\n",
      " [0.06896552]\n",
      " [0.10344828]\n",
      " [0.13793103]]\n"
     ]
    }
   ],
   "source": [
    "X_scaled = min_max_scaler(X)\n",
    "y_scaled = min_max_scaler(y)\n",
    "print(X_scaled[:5])\n",
    "print(\"======\")\n",
    "print(y_scaled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat:\n",
      "[0.4742546  0.47088419 0.46752178 0.46417296 0.46084327 0.45753813\n",
      " 0.45426283 0.45102252 0.44782214 0.44466646 0.44156002 0.43850709\n",
      " 0.43551173 0.4325777  0.42970851 0.42690734 0.42417713 0.42152049\n",
      " 0.41893976 0.41643696 0.41401385 0.41167187 0.40941222 0.40723581\n",
      " 0.4051433  0.40313508 0.40121133 0.39937198 0.39761678 0.39594525]\n",
      "======\n",
      "y:\n",
      "[0.         0.03448276 0.06896552 0.10344828 0.13793103 0.17241379\n",
      " 0.20689655 0.24137931 0.27586207 0.31034483 0.34482759 0.37931034\n",
      " 0.4137931  0.44827586 0.48275862 0.51724138 0.55172414 0.5862069\n",
      " 0.62068966 0.65517241 0.68965517 0.72413793 0.75862069 0.79310345\n",
      " 0.82758621 0.86206897 0.89655172 0.93103448 0.96551724 1.        ]\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network(1, 4, 1)\n",
    "y_hat = NN.forward(X_scaled)\n",
    "\n",
    "print(\"y_hat:\")\n",
    "print(y_hat.ravel())\n",
    "print(\"======\")\n",
    "print(\"y:\")\n",
    "print(y_scaled.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 再做 Back Propagation\n",
    "\n",
    "- 計算輸出層的 Loss\n",
    "- 將 Loss 輸入 sigmoid 導函數，計算 $W_1$ 的 Loss 比例\n",
    "- $W_1$ 的 Loss 再輸入 sigmoid 導函數，計算 $W_0$ 的 Loss 比例\n",
    "- 依據梯度遞減調整 $W_1$ 與 $W_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neural_Network:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "        self.W0 = np.random.randn(self._input_size, self._hidden_size)\n",
    "        self.W1 = np.random.randn(self._hidden_size, self._output_size)\n",
    "    def forward(self, X):\n",
    "        self.z0 = np.dot(X, self.W0)\n",
    "        self.z1 = self.sigmoid(self.z0)\n",
    "        self.z2 = np.dot(self.z1, self.W1)\n",
    "        output = self.sigmoid(self.z2)\n",
    "        return output\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x*(1-x)\n",
    "    def backward(self, X, y, output):\n",
    "        self.error = y - output\n",
    "        self.output_delta = self.error * self.sigmoid_derivative(output)\n",
    "        self.z1_error = np.dot(self.output_delta, self.W1.T)\n",
    "        self.z1_delta = self.z1_error * self.sigmoid_derivative(self.z1)\n",
    "        self.W0 += np.dot(X.T, self.z1_delta)\n",
    "        self.W1 += np.dot(self.z1.T, self.output_delta)\n",
    "    def train(self, X, y):\n",
    "        output = self.forward(X)\n",
    "        self.backward(X, y, output)\n",
    "    def get_weights(self):\n",
    "        return self.W0, self.W1\n",
    "    def predict(self, X_test):\n",
    "        return self.forward(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.2321717227043365\n",
      "Epoch: 1000, Loss: 0.03812162578616321\n",
      "Epoch: 2000, Loss: 0.036089745001124524\n",
      "Epoch: 3000, Loss: 0.03529837545376917\n",
      "Epoch: 4000, Loss: 0.03490766381521602\n",
      "Epoch: 5000, Loss: 0.034702067612926185\n",
      "Epoch: 6000, Loss: 0.03459881476904855\n",
      "Epoch: 7000, Loss: 0.03455858545240497\n",
      "Epoch: 8000, Loss: 0.03456000817600267\n",
      "Epoch: 9000, Loss: 0.03459032993787807\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network(1, 4, 1)\n",
    "for i in range(10000):\n",
    "    for x_, y_ in zip(X_scaled, y_scaled):\n",
    "        NN.train(x_.reshape(-1, 1), y_.reshape(-1, 1))\n",
    "    loss = np.mean(((y_scaled - NN.forward(X_scaled))**2).sum())\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Epoch: {}, Loss: {}\".format(i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.89827987  0.90333068 -0.89808679 -0.9035581 ]]\n",
      "[[-4.30576809]\n",
      " [ 9.52137165]\n",
      " [-3.89353405]\n",
      " [-6.67238022]]\n"
     ]
    }
   ],
   "source": [
    "weight_0, weight_1 = NN.get_weights()\n",
    "print(weight_0)\n",
    "print(weight_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.73841305]\n",
      " [ 9.45883124]\n",
      " [10.30406586]]\n",
      "[[5.]\n",
      " [7.]\n",
      " [9.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = X_scaled[:3]\n",
    "y_hat_scaled = NN.predict(X_test)\n",
    "y_hat = y.min() + (y.max() - y.min())*y_hat_scaled\n",
    "print(y_hat)\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 取得資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 簡單、作為測試目的即可\n",
    "\n",
    "Scikit-Learn Breast Cancer 資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "print(breast_cancer.feature_names)\n",
    "print(breast_cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "X_arr = breast_cancer.data\n",
    "y_arr = breast_cancer.target\n",
    "print(X_arr.shape)\n",
    "print(y_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 30)\n",
      "(171, 30)\n",
      "(398,)\n",
      "(171,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_arr, y_arr, test_size=0.3, random_state=123)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.93788057e+00  1.94992830e-01 -2.93534745e-03 -9.97160018e-03\n",
      "  -1.31053304e-01 -3.94404828e-01 -5.87789964e-01 -3.08240545e-01\n",
      "  -2.34487470e-01 -2.08028628e-02 -1.37757133e-03  1.25279091e+00\n",
      "   8.06843035e-03 -8.58257925e-02 -1.44335700e-02 -4.29282110e-04\n",
      "  -4.53672393e-02 -3.74242144e-02 -3.87957326e-02  6.72139227e-03\n",
      "   1.00102118e+00 -3.88323187e-01 -1.28745720e-01 -1.78899170e-02\n",
      "  -2.50197148e-01 -1.09822573e+00 -1.45824679e+00 -6.35170781e-01\n",
      "  -7.02595973e-01 -9.59181366e-02]]\n",
      "[0.41185323]\n",
      "0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Benchmark 完整程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X_arr = breast_cancer.data\n",
    "y_arr = breast_cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_arr, y_arr, test_size=0.3, random_state=123)\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 建構 TensorFlow 計算圖形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 準備 Placeholders 作為 Input layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "with tf.name_scope(\"input-layer\"):\n",
    "    X = tf.placeholder(tf.float32)\n",
    "    y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Placeholders 不指定外觀的好處\n",
    "\n",
    "- 多層感知器的應用常會使用 **Mini-batching** 的技巧\n",
    "- 訓練結束之後可以餵入 X_test 獲得 y_pred_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 準備 Variables 作為 Hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kuoyaojen/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_classes = np.unique(y_train).size\n",
    "n_neurons = 2**4\n",
    "with tf.name_scope(\"hidden-layer\"):\n",
    "    W = tf.Variable(tf.random_normal((n_features, n_neurons))) # (30, 16)\n",
    "    b = tf.Variable(tf.random_normal((n_neurons,)))            # (16,)\n",
    "    X_hidden = tf.nn.relu(tf.matmul(X, W) + b)                 # (398, 30) x (30, 16) + (16,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 如何決定 hidden-layer 的層數與神經元數量？\n",
    "\n",
    "- 絕大多數情境：使用一層 hidden-layer 就足夠\n",
    "- 神經元數量：$2^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 準備 Variables 作為 Output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"output-layer\"):\n",
    "    W = tf.Variable(tf.random_normal((n_neurons, 1))) # (16, 1)\n",
    "    b = tf.Variable(tf.random_normal((1,)))           # (1,)\n",
    "    y_logit = tf.squeeze(tf.add(tf.matmul(X_hidden, W), b))  # (398, 16) x (16, 1) + (1,)\n",
    "    y_one_prob = tf.sigmoid(y_logit)\n",
    "    y_pred = tf.round(y_one_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 寫下成本函數的公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y)\n",
    "    loss = tf.reduce_sum(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 宣告 Optimizer 與學習速率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kuoyaojen/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 建構 TensorFlow 計算圖形完整程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = np.unique(y_train).size\n",
    "n_neurons = 2**4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "with tf.name_scope(\"input-layer\"):\n",
    "    X = tf.placeholder(tf.float32)\n",
    "    y = tf.placeholder(tf.float32)\n",
    "with tf.name_scope(\"hidden-layer\"):\n",
    "    W = tf.Variable(tf.random_normal((n_features, n_neurons))) # (30, 16)\n",
    "    b = tf.Variable(tf.random_normal((n_neurons,)))            # (16,)\n",
    "    X_hidden = tf.nn.relu(tf.matmul(X, W) + b)                 # (398, 30) x (30, 16) + (16,)\n",
    "with tf.name_scope(\"output-layer\"):\n",
    "    W = tf.Variable(tf.random_normal((n_neurons, 1))) # (16, 1)\n",
    "    b = tf.Variable(tf.random_normal((1,)))           # (1,)\n",
    "    y_logit = tf.squeeze(tf.add(tf.matmul(X_hidden, W), b))  # (398, 16) x (16, 1) + (1,)\n",
    "    y_one_prob = tf.sigmoid(y_logit)\n",
    "    y_pred = tf.round(y_one_prob)\n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y)\n",
    "    loss = tf.reduce_sum(entropy)\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 539639.75\n",
      "step 1000, loss: 274796.96875\n",
      "step 2000, loss: 37507.2578125\n",
      "step 3000, loss: 5371.13427734375\n",
      "step 4000, loss: 5262.8720703125\n",
      "step 5000, loss: 5168.35546875\n",
      "step 6000, loss: 5027.861328125\n",
      "step 7000, loss: 4896.12841796875\n",
      "step 8000, loss: 3382.2109375\n",
      "step 9000, loss: 2716.5947265625\n",
      "step 10000, loss: 2126.1630859375\n",
      "step 11000, loss: 1527.1834716796875\n",
      "step 12000, loss: 977.72314453125\n",
      "step 13000, loss: 590.98681640625\n",
      "step 14000, loss: 299.52642822265625\n",
      "step 15000, loss: 104.7797622680664\n",
      "step 16000, loss: 68.5936279296875\n",
      "step 17000, loss: 59.4394416809082\n",
      "step 18000, loss: 52.51041030883789\n",
      "step 19000, loss: 47.401100158691406\n",
      "step 20000, loss: 43.703914642333984\n",
      "step 21000, loss: 40.82254409790039\n",
      "step 22000, loss: 38.577735900878906\n",
      "step 23000, loss: 36.7269172668457\n",
      "step 24000, loss: 35.05070495605469\n",
      "step 25000, loss: 33.61341094970703\n",
      "step 26000, loss: 32.35951232910156\n",
      "step 27000, loss: 31.277803421020508\n",
      "step 28000, loss: 30.297021865844727\n",
      "step 29000, loss: 29.438457489013672\n",
      "step 30000, loss: 28.668413162231445\n",
      "step 31000, loss: 27.969507217407227\n",
      "step 32000, loss: 27.324005126953125\n",
      "step 33000, loss: 26.739540100097656\n",
      "step 34000, loss: 26.172687530517578\n",
      "step 35000, loss: 25.6906795501709\n",
      "step 36000, loss: 25.1654109954834\n",
      "step 37000, loss: 24.71441078186035\n",
      "step 38000, loss: 24.274803161621094\n",
      "step 39000, loss: 23.863950729370117\n",
      "step 40000, loss: 23.47426986694336\n",
      "step 41000, loss: 23.104055404663086\n",
      "step 42000, loss: 22.750202178955078\n",
      "step 43000, loss: 22.418256759643555\n",
      "step 44000, loss: 22.089771270751953\n",
      "step 45000, loss: 21.784543991088867\n",
      "step 46000, loss: 21.489147186279297\n",
      "step 47000, loss: 21.211597442626953\n",
      "step 48000, loss: 20.97308921813965\n",
      "step 49000, loss: 20.731098175048828\n"
     ]
    }
   ],
   "source": [
    "n_steps = 50000\n",
    "file_writer_path = \"./graphs/fully-connected-deep-networks\"\n",
    "loss_history = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # 初始化所有的變數張量！\n",
    "    train_writer = tf.summary.FileWriter(file_writer_path, tf.get_default_graph())\n",
    "    for i in range(n_steps):\n",
    "        feed_dict = {\n",
    "            X: X_train,\n",
    "            y: y_train\n",
    "        }\n",
    "        _, loss_ = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        loss_history.append(loss_)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"step {}, loss: {}\".format(i, loss_))\n",
    "    w_final, b_final = sess.run([W, b])\n",
    "    y_pred_arr = sess.run(y_pred, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(n_steps), loss_history)\n",
    "plt.title(\"Loss Summary\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26682755]\n",
      " [-0.53566235]\n",
      " [-0.10864915]\n",
      " [ 1.4385455 ]\n",
      " [-0.6868179 ]\n",
      " [-1.319403  ]\n",
      " [ 0.5819109 ]\n",
      " [ 0.06051218]\n",
      " [ 0.2184967 ]\n",
      " [ 0.6089343 ]\n",
      " [-0.143741  ]\n",
      " [-1.5354853 ]\n",
      " [-0.6753923 ]\n",
      " [-1.8530135 ]\n",
      " [ 1.4804659 ]\n",
      " [ 0.6578026 ]]\n",
      "[2.2221074]\n",
      "0.9766081871345029\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred_arr)\n",
    "print(w_final)\n",
    "print(b_final)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 隨堂練習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 使用 titanic 資料集並利用 TensorFlow 建立一個 Fully Connected Deep Networks Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_url = \"https://s3-ap-northeast-1.amazonaws.com/kaggle-getting-started/titanic/train.csv\"\n",
    "test_url = \"https://s3-ap-northeast-1.amazonaws.com/kaggle-getting-started/titanic/test.csv\"\n",
    "train_df = pd.read_csv(train_url)\n",
    "test_df = pd.read_csv(test_url)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
